{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b635c04c",
   "metadata": {},
   "source": [
    "### Kullback-Leibler Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45031fe5",
   "metadata": {},
   "source": [
    "For two distributions q (the one I have) and p (the one I want to match), the KL divergence measures how different they are.  \n",
    "\n",
    "KL(q||p) = sum_i(q_i * log(q_i/p_i))\n",
    "\n",
    "It's always >= 0, and 0 only when q = p  \n",
    "It's asymmetric: KL(q||p) != KL(p||q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964aae7e",
   "metadata": {},
   "source": [
    "This is required to train the prior network when there are observations being passed through. The posterior network uses the observation and produces a distribution over latent states conditioned on it, while the prior network predicts the latent distribution purely from its own recurrent state (i.e., without seeing the observation). The KL divergence between these two distributions measures how far the prior's prediction is from the conditioned posterior network. \n",
    "\n",
    "During training, minimizing KL updates the prior's networks weights so that its predicted distribution better matches the posterior's prediction, which is basically teaching the prior to infer what the posterior would for future observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7e1bac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1031, 3.5609])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(44)\n",
    "# (1) will take both logits from the posterior and prior networks\n",
    "# create some mock data\n",
    "postr = torch.randn(2, 4, 4)\n",
    "prior = torch.randn(2, 4, 4)\n",
    "\n",
    "# (2) apply log softmax to both\n",
    "\n",
    "# manual implementation\n",
    "def log_softmax(x, dim=-1):\n",
    "    return x - torch.log(torch.sum(torch.exp(x), dim=dim, keepdim=True))\n",
    "\n",
    "q_log = log_softmax(postr)\n",
    "p_log = log_softmax(prior)\n",
    "\n",
    "# (3) also apply .exp() to the output of log_softmax(q) to get q\n",
    "q = q_log.exp()\n",
    "\n",
    "# (4) compute kl \n",
    "# we want a value that represents how *surprised* the prior network is by the posterior's dist for that specific obs\n",
    "# double sum to get this value because want (B, G, C) -> (B)\n",
    "# kl ~ 0 is when the posterior ~ prior\n",
    "# kl > 0 (moderate) posterior is somewhat different \n",
    "# kl > 0 (large) posterior is VERY different -> big update req \n",
    "kl = (q * (q_log - p_log)).sum(dim=-1).sum(dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e8c5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use as a function and using pytorch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _kl(posterior_logits, prior_logits):\n",
    "    q_log = F.log_softmax(posterior_logits, dim=-1)       # (B, G, C)\n",
    "    p_log = F.log_softmax(prior_logits, dim=-1)           # (B, G, C)\n",
    "    q     = q_log.exp()\n",
    "    kl    = (q * (q_log - p_log)).sum(dim=-1).sum(dim=-1) # (B)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f8747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
