{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671b6225",
   "metadata": {},
   "source": [
    "### Learning GRU layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1c254",
   "metadata": {},
   "source": [
    "### Math formulas:\n",
    "- **update gate z_t:**                                  \n",
    "z_t​ = sigmoid(W_z​ @ x_t ​+ U_z @ ​h_prev​)  \n",
    "\n",
    "- **reset gate r_t:**                                   \n",
    "r_t = sigmoid(W_r @ x_t + U_r @ h_prev)  \n",
    "\n",
    "- **candidate hidden state h_t_candidate:**             \n",
    "tanh(W_h @ x_t + U_h @ (r_t * h_prev))  \n",
    "\n",
    "- **final hidden state h_t:**                           \n",
    "h_t = (1 - z_t) * h_prev + z_t * h_t_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5986e",
   "metadata": {},
   "source": [
    "Begin by understanding what kinds of inputs go into a GRU layer:  \n",
    "\n",
    "The input is x_t which is a vector that contains d_x number of features. It is supposed to represent a specific *time* step x_t.  \n",
    "\n",
    "If all the time steps of x_t are taken together, we get a matrix with shape T x n_features, where T is the whole sequence length (little t denotes one particular time step; T denotes the whole sequence). \n",
    "\n",
    "Normally, it is processed in batches, so we'd get an input shape of (B, T, n_features). This is what is going into the GRU layer.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaff2ad",
   "metadata": {},
   "source": [
    "The GRU cell handles each x_t independently. At each timestep t, the GRU cell gets the whole batch and all features, so the GRU layer needs the amount T GRU cells to process the whole sequence. Because of this, the sample input is actually of shape (T, B, n_features) because each x_t of T is processed by each cell independently. However, in pytorch, I have still noticed that B is commonly used first.\n",
    "\n",
    "Also, going forward, n_features is usually denoted as d_x to represent the *dimension of the input vector*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50add8",
   "metadata": {},
   "source": [
    "Furthermore, there is a hidden dimension associated with the GRU cells. This is a hyperparameter that determines the model memory/context. It basically is a compressed summary of all the info seen up to x_t. It is what carries the information forward to use as a basis for the next prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b705a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "torch.manual_seed(44)\n",
    "\n",
    "# example input x_t that I will be working with\n",
    "# ensuring that easy numbers will be used\n",
    "\n",
    "B, T = 1, 3          # batch size, timesteps\n",
    "d_x, d_h = 4, 3      # input features per step, hidden size\n",
    "\n",
    "# manual sigmoid\n",
    "# NOTE: can't use on tensors\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# manual tanh\n",
    "# NOTE: can't use on tensors\n",
    "def tanh(x):\n",
    "    return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\n",
    "\n",
    "\n",
    "# random sample of data\n",
    "X = torch.randn(B, T, d_x)\n",
    "\n",
    "t = 0 # pick the initial t just for example\n",
    "x_t = X[:, t, :] # an input at a single time step \n",
    "\n",
    "# init h_prev\n",
    "h_prev = torch.zeros(B, d_h)\n",
    "\n",
    "# a weight matrix of size d_h x d_x for W's because x_t is d_x for the last shape\n",
    "# when doing x @ W, W is expected to be d_x by d_h but nn.Linear does it this way\n",
    "# so have to remember to do x @ W.T\n",
    "# the W matrices map the input onto the hidden state d_h; they project it onto d_h\n",
    "# takes the input info and initializes it into the hidden state\n",
    "W_z = torch.randn(d_h, d_x) \n",
    "\n",
    "# U_z operates on h_prev, which is of shape B x d_h\n",
    "# so U_z must have d_h because the hidden state is in R**d_h\n",
    "# this conceptually makes sense because if d_x is taken again then it would be treating the hidden state as another input\n",
    "# the U matrices take the old info and project it into the current state\n",
    "# 'reprocesses' the memory of the past before combining with the new input\n",
    "U_z = torch.randn(d_h, d_h)\n",
    "\n",
    "W_r = torch.randn(d_h, d_x)\n",
    "U_r = torch.randn(d_h, d_h)\n",
    "\n",
    "W_h = torch.randn(d_h, d_x)\n",
    "U_h = torch.randn(d_h, d_h)\n",
    "\n",
    "\n",
    "# need four formulas coded up\n",
    "# (1) update gate\n",
    "z_t = torch.sigmoid(x_t @ W_z.T + h_prev @ U_z.T)\n",
    "\n",
    "# (2) reset gate\n",
    "r_t = torch.sigmoid(x_t @ W_r.T + h_prev @ U_r.T)\n",
    "\n",
    "# (3) candidate hidden state\n",
    "h_t_tilda = torch.tanh(x_t @ W_h.T + (r_t * h_prev) @ U_h.T)\n",
    "\n",
    "# (4) final hidden state\n",
    "h_t = (1 - z_t) * h_prev + z_t * h_t_tilda\n",
    "\n",
    "h_prev = h_t # set h_prev to the new h_t for the next iteration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdca246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# code it up as pytorch does it (no bias)\n",
    "class GRUCell(torch.nn.Module):\n",
    "    def __init__(self, d_h, d_x):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_h, self.d_x = d_h, d_x\n",
    "\n",
    "        # stacked weights for input-to-hidden\n",
    "        self.weights_ih = torch.nn.Parameter(torch.randn(3 * d_h, d_x)) # -> (3 * d_h, d_x) -> (9, 4)\n",
    "\n",
    "        # stacked weights for hidden-to-hidden\n",
    "        self.weights_hh = torch.nn.Parameter(torch.randn(3 * d_h, d_h)) # (9, 3)\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "\n",
    "        # torch uses linear function to perform x @ W.T + b (if bias True)\n",
    "\n",
    "        # gates from input\n",
    "        gi = F.linear(x_t, self.weights_ih) # (B, d_x) @ (d_x, 3 * d_h) -> (B, 3 * d_h)\n",
    "\n",
    "        # gates from hidden\n",
    "        # NOTE: for h_t_tilda, h_prev @ U_h.T is calc before h_prev is multiplied with r_t; gives same results\n",
    "        gh = F.linear(h_prev, self.weights_hh) # (B, d_h) @ (d_h, 3 * d_h) -> (B, 3 * dh)\n",
    "\n",
    "        i_r, i_z, i_n = gi.chunk(3, dim=1) # 3 * (B, d_h)\n",
    "        h_r, h_z, h_n = gh.chunk(3, dim=1) # 3 * (B, d_h)\n",
    "\n",
    "        z_t = torch.sigmoid(i_z + h_z) # (B, d_h) + (B, d_h) \n",
    "        r_t = torch.sigmoid(i_r + h_r)\n",
    "        n_t = torch.tanh(i_n + r_t * h_n)\n",
    "\n",
    "        h_t = (1 - z_t) * h_prev + z_t * n_t # original formula\n",
    "        # NOTE: pytorch does this differently, i.e., h_t = (1 - z_t) * n_t + z_t * h_prev\n",
    "        # from what I've gathered, they want more 'past' info to carry over\n",
    "        # e.g., if z_t is big, we get more of the h_prev info and less of the new candidate info\n",
    "        # the original formula does the opposite, i.e., if z_t is big, then more of the new candidate info\n",
    "        # HOWEVER, if the model is being trained, it doesn't matter because the weights will update accordingly\n",
    "        # it is ultimately done because of efficiency reasons due to cuDNN, which implements GRU via this new formula\n",
    "        # so it keeps consistency with CUDA kernels for optimization\n",
    "\n",
    "        return h_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, d_h, d_x = 1, 3, 3, 4\n",
    "X = torch.randn(B, T, d_x)\n",
    "cell = GRUCell(d_h, d_x)\n",
    "h_prev = torch.zeros(B, d_h)\n",
    "\n",
    "hs = []\n",
    "for t in range(T):\n",
    "    x_t = X[:, t, :]\n",
    "    h_prev = cell(x_t, h_prev)\n",
    "    hs.append(h_prev)\n",
    "\n",
    "\n",
    "H = torch.stack(hs, dim=1) # (B, T, d_h)\n",
    "# stack creates new dim; concat adds along existing dim\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9373c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6569, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8bf4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
