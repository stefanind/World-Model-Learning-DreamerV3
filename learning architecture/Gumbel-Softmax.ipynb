{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec76b293",
   "metadata": {},
   "source": [
    "# Learning Gumbel-Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d9af1",
   "metadata": {},
   "source": [
    "DreamerV3 compresses images into discrete representations. This means that the image is encoded into a number of categorical groups, each with a certain number of possible values that it can take. For example, a category can be color, shape, and/or size. If we take the color category, then the possible values are red, green, blue, yellow. A one-hot vector is used to represent the encoded choice, e.g., [1,0,0,0] for red. \n",
    "\n",
    "An issues arises when sampling a categorical variable though. It would typically require the argmax of the probabilities (softmaxed) for the choice and then to represent this choice as a one-hot vector, but torch.argmax isn't differentiable so backprop can't run through it in training. Gumbel-Softmax resolves this issue. \n",
    "\n",
    "How does it work?\n",
    "\n",
    "Taking argmax always gives the most likely class, which is deterministic, something we're not after because we are including uncertainty (stochastic state). So we need to inject *Gumbel noise* before this argmax to add some uncertainty. But we can't use argmax, so softmax is used, a different formula that contains only exponentials, additions, and divisions, all differentiable operations. However, since we're adding uncertainty to this, we used what is called the Gumbel-Softmax. The main difference is that there is the added noise and a variable called *temperature*. The noise is to add randomness but the temperature is to get the argmax behavior to get the one-hot vectors. Basically, as the temperature approaches 0, the closer we get to a one-hot vector. And as temperature approaches neg infinity, the outputs become close to uniform. \n",
    "\n",
    "How gumbel_softmax works in pytorch:  \n",
    "Pass the logits, the temperature, and hard=True or False. If hard is set to True, it returns a one-hot vector (like argmax) and the gradients from the softmax version. If hard is set to False, it won't return the one-hot vector, only the purely softmaxed output probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "758aea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual implementation of gumbel_softmax\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(44)\n",
    "\n",
    "# sample logits\n",
    "logits = torch.tensor([2.0, 0.5, -1.0])\n",
    "\n",
    "# temperature (tau)\n",
    "tau = 0.5\n",
    "\n",
    "shape = logits.shape\n",
    "\n",
    "# Gumbel noise sampler\n",
    "# g = -log(-log(u)) with u ~ Uniform(0, 1)\n",
    "eps = 1e-20 # to prevent blowing up, e.g., log(0) -> neg inf; 1e-20 effectively is zero at float precision\n",
    "u = torch.rand(shape) # rand samples from Uniform[0, 1)\n",
    "g = -torch.log(-torch.log(u + eps) + eps)\n",
    "\n",
    "# softmax_x_i = exp(x_i) / sum_j(exp(x_j))\n",
    "soft = torch.softmax((logits + g) / tau, dim=-1)\n",
    "\n",
    "# hard\n",
    "idx = torch.argmax(soft, dim=-1, keepdim=True) # gets indices from the last dim and keeps the dimension\n",
    "hard = torch.zeros_like(soft).scatter_(dim=-1, index=idx, value=1.0) # create a tensor of same size and modify in place\n",
    "\n",
    "# THE important step for how gumbel-softmax works in pytorch\n",
    "# when passing gradients through, only 'soft' has a gradient\n",
    "# soft.detach() is treated as a constant because of detach()\n",
    "# hard doesn't have a gradient because of argmax\n",
    "# so only 'soft' passes its gradient through\n",
    "# resolves the issue by giving a one hot vector in the forward pass for the discrete representation and training the model through the soft calculation\n",
    "y = hard - soft.detach() + soft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d2eec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 4, 4]), tensor([0., 0., 1., 0.], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# how it works in the RSSM:\n",
    "\n",
    "# dummy settings\n",
    "B = 8\n",
    "num_groups  = 4\n",
    "num_classes = 4\n",
    "size_deterministic      = 64\n",
    "size_obs_embed          = 32\n",
    "size_stochastic_flat    = num_groups * num_classes\n",
    "# dummy inputs\n",
    "deterministic_state_t   = torch.randn(B, size_deterministic)\n",
    "observation_embedding_t = torch.randn(B, size_obs_embed)\n",
    "# posterior head in RSSM\n",
    "# takes in obs embedding and the hidden state and produces logits of the size num_groups * num_classes\n",
    "posterior_head = nn.Linear(size_deterministic + size_obs_embed, size_stochastic_flat)\n",
    "joined = torch.cat([deterministic_state_t, observation_embedding_t], dim=-1)\n",
    "group_class_logits = posterior_head(joined)\n",
    "# gumbel softmax in pytorch expects 2D tensor\n",
    "# so multiply B * G to get the number of independent categorical distributions, each with C classes per distribution\n",
    "# allows the gumbel_softmax to treat each (B, G) pair independently and samples a one hot vector for each. \n",
    "x = group_class_logits.view(B*num_groups, num_classes)\n",
    "onehots = F.gumbel_softmax(x, tau=0.5, hard=True, dim=-1)\n",
    "out = onehots.view(B, num_groups, num_classes)\n",
    "out.shape, out[0, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64921a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
