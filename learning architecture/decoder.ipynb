{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b71740ad",
   "metadata": {},
   "source": [
    "# Learning the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e5de1",
   "metadata": {},
   "source": [
    "For the convolutional decoder, we're initially working with discrete representations, which the convolutions do not work with, they require a continuous representation. Therefore, I have to convert these discrete reps to continuous ones. The method used is by embedding the categorical variables (the groups that contain the n different classes).  \n",
    "\n",
    "I get an embedding for each single class that is found within a group (a category, such as 'color'). So if there were 4 colors (red, blue, yellow, green), each of these colors contains an embedding that is of a size of our choosing. Each group has their own embedding created. So if there were 3 groups (color, shape, texture), each of these would have their own nn.Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27060ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13a21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For only one sample:\n",
      "For group 1, 0 is the index associated with the chosen class.\n",
      "For group 2, 1 is the index associated with the chosen class.\n",
      "For group 3, 3 is the index associated with the chosen class.\n",
      "For group 4, 2 is the index associated with the chosen class.\n"
     ]
    }
   ],
   "source": [
    "# --- what we're expecting as input ---\n",
    "# we want the integer indices associated with each group, e.g., if there were 4 groups each with 4 classes to pick from\n",
    "# then we'd have something like the following:\n",
    "x = torch.tensor([0, 1, 3, 2])\n",
    "print(\"For only one sample:\")\n",
    "for i in range(len(x)):\n",
    "    print(f\"For group {i+1}, {x[i]} is the index associated with the chosen class.\")\n",
    "\n",
    "# normally there are batches, so we change the input as such if there are 3 samples in one batch:\n",
    "y = torch.tensor([[0, 1, 3, 2],\n",
    "                  [1, 2, 3, 0],\n",
    "                  [1, 1, 2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- where the input comes from ---\n",
    "# from the posterior (Encoder) or prior (Dynamics Predictor) network, we get logits\n",
    "# these logits are used to find the biggest one for each group\n",
    "# e.g., if one group (G) has 4 classes (C), then there will be 4 logits, each assoc with a class\n",
    "# the largest logit will be chosen, which would be the discrete choice,\n",
    "# i.e., the largest logit is associated with the 1 from theone hot vector that would be chosen via the gumbel softmax\n",
    "\n",
    "# --- EXAMPLE ---\n",
    "# logits = dynamics.dist(h_t) # dynamics is the instantiated class and .dist() is the method that gets the logits\n",
    "# z_idx  = logits.argmax(dim=-1)\n",
    "# z_idx\n",
    "# torch.tensor([[0, 1, 3, 2],\n",
    "#               [1, 2, 3, 0],\n",
    "#               [1, 1, 2, 2]])\n",
    "# this would give (B, G) size with each G being an index that picks out the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f89b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 1, 3, 2],\n",
    "               [1, 2, 3, 0],\n",
    "               [1, 1, 2, 2]])\n",
    "\n",
    "n_classes = 4\n",
    "embed_dim = 16\n",
    "n_cats    = 4\n",
    "batch     = 3\n",
    "\n",
    "tables = nn.ModuleList([nn.Embedding(n_classes, embed_dim) for _ in range(n_cats)])\n",
    "\n",
    "pieces = []\n",
    "for i, tab in enumerate(tables):\n",
    "    pieces.append(tab(x[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da1f439f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6862, -0.4291,  0.8057, -0.0298, -0.7715,  1.0482, -0.5261,  1.4336,\n",
       "          1.0581,  2.6074,  0.0080,  1.7126,  1.1000,  1.2689, -0.1735,  1.5911],\n",
       "        [-0.4210, -0.5019, -0.1908, -0.8345, -0.7349,  0.5490,  0.4359,  0.8548,\n",
       "          0.9642,  1.3847, -0.1931,  0.3786,  1.2145,  0.6541,  0.2604,  0.4738],\n",
       "        [-0.4210, -0.5019, -0.1908, -0.8345, -0.7349,  0.5490,  0.4359,  0.8548,\n",
       "          0.9642,  1.3847, -0.1931,  0.3786,  1.2145,  0.6541,  0.2604,  0.4738]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pieces[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2266a3",
   "metadata": {},
   "source": [
    "So in the example above, there is an embedding of 16 dimensions for each associated discrete representation. pieces[0] contains each embedding for the first row in x (i.e., [0, 1, 3, 2]). So the 0 in the first row of x has a 16 dimension embedding attached to it, which is the continuous representation of that discrete one created earlier from the encoder. There are embeddings attached to each of these numbers in x, making up a total of 4 * 3 * 16 embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01c6d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch implementation\n",
    "\n",
    "class DiscreteEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cats, n_classes, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tables = nn.ModuleList([\n",
    "            nn.Embedding(n_classes, embed_dim) for _ in range(n_cats)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        pieces = [tab(x[:, i]) for i, tab in enumerate(self.tables)]\n",
    "        return torch.cat(pieces, dim=-1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1261679",
   "metadata": {},
   "source": [
    "TODO: finish decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42099eeb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
