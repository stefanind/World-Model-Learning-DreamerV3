{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8e0838",
   "metadata": {},
   "source": [
    "# Learning Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96cc5b",
   "metadata": {},
   "source": [
    "The DreamerV3 encoder, depending on if the input gives pixels, is a convolutional encoder. The encoder will transform the high dim image into a lower dim *latent representation* (in this case, a discrete, compressed representation; see https://arxiv.org/abs/2312.01203 regarding discrete vs continuous representations). This latent representation is what the RSSM uses. An MLP encoder is used for other kinds of inputs, such as state vectors (e.g., positions, velocities, etc.). Robotics tasks will use both (camera input and state vectors).\n",
    "\n",
    "So initially, I will focus on convolutional encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c7eee",
   "metadata": {},
   "source": [
    "A convolutional is basically a learnable filter (its weights). A *filter kernel* slides over the input and computes the dot product at each spatial location. More specifically, in this \"spatial location,\" the kernel computes a dot product between the filter weights and the patch of the input it sits over, giving a value for the whole patch. The goal that is optimized here is whether the dot product is associated with what is wanted, i.e., it basically says \"does this patch look like the filter.\" If yes, the weights will be nudged further because it did accurately capture the patch (change the weights to make the dot product larger). If no, the weights are nudged the other way so the filter stops firing that way (change the weights to make the dot product lower). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9cb8f",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8365f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# code up a basic convolution to get the idea across\n",
    "\n",
    "# example image \n",
    "x = torch.arange(25, dtype=torch.float32).reshape(5,5)\n",
    "\n",
    "# a 3x3 kernel w/ bias\n",
    "# the kernel and bias are learnable parameters\n",
    "kernel = torch.randn(3,3)\n",
    "bias   = torch.randn(1)\n",
    "\n",
    "# output after the kernel (3x3)\n",
    "out_h, out_w = x.shape[0] - 3 + 1, x.shape[1] - 3 + 1\n",
    "out = torch.zeros(out_h, out_w)\n",
    "\n",
    "# basic convolution (no stride, no padding, single channel--no color)\n",
    "for i in range(out_h):\n",
    "    for j in range(out_w):\n",
    "        patch = x[i:i+3, j:j+3] # i+3 and j+3 to make a square of size 3x3\n",
    "        out[i, j] = torch.sum(patch * kernel) + bias # dot product + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc29a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch implementation\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size, kernel_size))\n",
    "        self.bias   = nn.Parameter(torch.randn(1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape\n",
    "        k = self.weight.shape[0] # k is a hyperparameter (kernel_size)\n",
    "        out_h, out_w = h - k + 1, w - k + 1\n",
    "        out = torch.zeros(out_h, out_w)\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                patch = x[i:i+k, j:j+k]\n",
    "                out[i, j] = torch.sum(patch * self.weight) + self.bias\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7f1f9",
   "metadata": {},
   "source": [
    "### with stride and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.arange(25, dtype=torch.float32).reshape(5,5)\n",
    "kernel = torch.randn(3,3)\n",
    "bias   = torch.randn(1)\n",
    "\n",
    "\n",
    "padding = 1\n",
    "\n",
    "if padding > 0:\n",
    "    x = torch.nn.functional.pad(x, (padding, padding, padding, padding)) # use pad to add zeros around the border\n",
    "\n",
    "\n",
    "# using stride \n",
    "stride = 2 # how far the kernel shifts after computing dot product for one position\n",
    "out_h = x.shape[0] - kernel.shape[0] // stride + 1\n",
    "out_w = x.shape[1] - kernel.shape[1] // stride + 1\n",
    "out = torch.zeros(out_h, out_w)\n",
    "\n",
    "# basic convolution (no stride, no padding, single channel--no color)\n",
    "for i in range(out_h):\n",
    "    for j in range(out_w):\n",
    "        patch = x[i*stride:i*stride+kernel.shape[0], j*stride:j*stride+kernel.shape[1]]\n",
    "        out[i, j] = torch.sum(patch * kernel) + bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419528aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch implementation\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size, kernel_size))\n",
    "        self.bias   = nn.Parameter(torch.randn(1))\n",
    "\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.padding > 0:\n",
    "            x = torch.nn.functional.pad(x, (self.padding, self.padding, self.padding, self.padding))\n",
    "        \n",
    "        h, w = x.shape\n",
    "        k = self.weight.shape[0] # k is a hyperparameter (kernel_size)\n",
    "        out_h = h - k // self.stride + 1\n",
    "        out_w = w - k // self.stride + 1\n",
    "        out = torch.zeros(out_h, out_w)\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                patch = x[i*self.stride:i*self.stride+k, j*self.stride:j*self.stride+k]\n",
    "                out[i, j] = torch.sum(patch * self.weight) + self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ae977",
   "metadata": {},
   "source": [
    "### Conv Encoder for DreamerV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 3, model_dim: int = 1024):\n",
    "        super().__init__()\n",
    "\n",
    "        chs = max(16, model_dim // 16)\n",
    "\n",
    "        # want to increase the feature channel (dim=1) as we compress the image (dim 2 and 3)\n",
    "        # we begin with (B, feature_chs, H, W)\n",
    "        # want feature_chs to increase to incorporate more semantic meaning\n",
    "        # while reducing the visual map by compression so that semantic wholeness is acquired\n",
    "        # i.e., if we compress the image enough, the weights have to learn more 'bigger picture' features\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,   chs,   kernel_size=4, stride=2, padding=1), nn.SiLU(), \n",
    "            nn.Conv2d(chs,           chs*2, kernel_size=4, stride=2, padding=1), nn.SiLU(),     \n",
    "            nn.Conv2d(chs*2,         chs*4, kernel_size=4, stride=2, padding=1), nn.SiLU(),\n",
    "            nn.Conv2d(chs*4,         chs*8, kernel_size=4, stride=2, padding=1), nn.SiLU()\n",
    "        )\n",
    "        # we expect the flattened output, e.g., all dimensions except the batch multiplied together\n",
    "        # assumes that the end output will have H, W = 4, 4\n",
    "        self.proj = nn.Linear(8 * chs * 4 * 4, model_dim)\n",
    "\n",
    "    def forward(self, x): # expects a 64 x 64 img\n",
    "        h = self.conv(x)\n",
    "        if h.shape[-1] != 4 or h.shape[-2] != 4:\n",
    "            h = F.adaptive_avg_pool2d(h, (4,4))\n",
    "        # we need to flatten before puttign into linear\n",
    "        h = h.flatten(1) # flatten from dim=1 to dim=-1\n",
    "        logits = self.proj(h)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "721a5ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = ConvEncoder()\n",
    "x = torch.randn(3, 3, 64, 64)\n",
    "conv(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add09d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
